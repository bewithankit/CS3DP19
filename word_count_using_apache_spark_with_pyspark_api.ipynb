{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgzPXZXw0yeopTN9yl/NBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bewithankit/CS3DP19/blob/main/word_count_using_apache_spark_with_pyspark_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Set Up the Environment:**\n",
        "\n",
        "\n",
        "*   First, you need to install all the necessary components to run Spark in Colab. This includes Java, Spark itself, and findspark, a Python library that makes it easier to find Spark.\n",
        "\n",
        "*   Use the `!` operator to run the following commands in a Colab cell to install Java and Spark, and set up the environment variables:"
      ],
      "metadata": {
        "id": "UD1qmJ9PR0PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "Hl14k-BdR5Rl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Then, set the environment variables for Java and Spark in Python:\n",
        "\n"
      ],
      "metadata": {
        "id": "sC209GUFTcpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n"
      ],
      "metadata": {
        "id": "yAY9re_gSATo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Initialize Spark:**\n",
        "\n",
        "\n",
        "*   Import `findspark` and initialize it. Then create a SparkContext.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XsF8mIFwSTG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName('wordCount')\n",
        "sc = SparkContext(conf=conf)\n",
        "# sc = SparkContext.getOrCreate(conf=conf)\n"
      ],
      "metadata": {
        "id": "4S6cFFHjSQgC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Create an RDD:**\n",
        "\n",
        "* Create a base RDD from a simple list of words and parallelize it."
      ],
      "metadata": {
        "id": "EFRbqnyRSVdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordsList = ['blue', 'orange', 'red', 'blue', 'red', 'blue']\n",
        "wordsRDD = sc.parallelize(wordsList, 5)\n",
        "print(type(wordsRDD))  # Should print <class 'pyspark.rdd.RDD'>\n",
        "\n",
        "# Use glom to see how the data is distributed across partitions\n",
        "glommedRDD = wordsRDD.glom()\n",
        "print(glommedRDD.collect())"
      ],
      "metadata": {
        "id": "fH6XFPJtSZzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d8f59c-cba7-4642-cb7b-52d6347425e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Transform the RDD:**\n",
        "\n",
        "* Use a map operation to transform each word in the RDD."
      ],
      "metadata": {
        "id": "5RGCwg2uSb-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a defined function\n",
        "def webify(x):\n",
        "    return x+\".com\"\n",
        "webified_RDD = wordsRDD.map(webify)\n",
        "print(webified_RDD.collect())\n",
        "\n",
        "# Using a lambda function # lambda <args>: <method body>\n",
        "webified_RDD = wordsRDD.map(lambda word: word + '.com')\n",
        "print(webified_RDD.collect())\n"
      ],
      "metadata": {
        "id": "ky5wMR8DSgc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "592a0635-3e21-4a7f-c0ab-38ca8b5eb344"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['blue.com', 'orange.com', 'red.com', 'blue.com', 'red.com', 'blue.com']\n",
            "['blue.com', 'orange.com', 'red.com', 'blue.com', 'red.com', 'blue.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Create Pair RDDs:**\n",
        "\n",
        "* Map each word to a tuple containing the word and the number 1, to prepare for counting."
      ],
      "metadata": {
        "id": "Z2R2uXnxSuZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordPairs = wordsRDD.map(lambda word: (word, 1))\n",
        "print(wordPairs.collect())\n"
      ],
      "metadata": {
        "id": "fG-l96ImSlsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69a9172-a356-413e-9380-424a47dd1c7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('blue', 1), ('orange', 1), ('red', 1), ('blue', 1), ('red', 1), ('blue', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Count the Words:**\n",
        "\n",
        "* Two approaches can be used: `groupByKey()` or `reduceByKey()`. The latter is more efficient."
      ],
      "metadata": {
        "id": "RIn4FGJrSn8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using groupByKey()\n",
        "wordsGrouped = wordPairs.groupByKey()\n",
        "for key, value in wordsGrouped.collect():\n",
        "  print ('{0}: {1}'.format(key, list(value)))\n",
        "\n",
        "wordCountsGrouped = wordsGrouped.mapValues(sum)\n",
        "print(wordCountsGrouped.collect())\n",
        "\n",
        "# Using reduceByKey()\n",
        "wordCounts = wordPairs.reduceByKey(lambda x, y: x + y)\n",
        "print(\"\\n\")\n",
        "print(wordCounts.collect())\n"
      ],
      "metadata": {
        "id": "7f-4QNOySqzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77c2cc6-ba8c-4de5-a1b3-d7b98f3bbb1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orange: [1]\n",
            "red: [1, 1]\n",
            "blue: [1, 1, 1]\n",
            "[('orange', 1), ('red', 2), ('blue', 3)]\n",
            "\n",
            "\n",
            "[('orange', 1), ('red', 2), ('blue', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Run the Complete Application:**\n",
        "\n",
        "* Combine all the steps into a single sequence to count the words."
      ],
      "metadata": {
        "id": "E3S7xvfXS0JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordCountsCollected = (wordsRDD\n",
        "                       .map(lambda word: (word, 1))\n",
        "                       .reduceByKey(lambda x, y: x + y)\n",
        "                       .collect())\n",
        "print(wordCountsCollected)\n"
      ],
      "metadata": {
        "id": "AQ4HAL7NS6YS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d8fd6e-754a-4b19-ebd0-285cd1a5d88e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('orange', 1), ('red', 2), ('blue', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Shutting Down:**\n",
        "\n",
        "* After you are done, stop the SparkContext to free up resources."
      ],
      "metadata": {
        "id": "oAiOlf-HS9ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "nXYlzfNVUw9s"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}