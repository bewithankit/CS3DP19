{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm5quBAoaViOyfhsDiO6Kt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bewithankit/CS3DP19/blob/main/word_count_using_apache_spark_with_pyspark_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Set Up the Environment:**\n",
        "\n",
        "\n",
        "*   First, you need to install all the necessary components to run Spark in Colab. This includes Java, Spark itself, and findspark, a Python library that makes it easier to find Spark.\n",
        "\n",
        "*   Use the `!` operator to run the following commands in a Colab cell to install Java and Spark, and set up the environment variables:"
      ],
      "metadata": {
        "id": "UD1qmJ9PR0PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "Hl14k-BdR5Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Then, set the environment variables for Java and Spark in Python:\n",
        "\n"
      ],
      "metadata": {
        "id": "sC209GUFTcpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n"
      ],
      "metadata": {
        "id": "yAY9re_gSATo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Initialize Spark:**\n",
        "\n",
        "\n",
        "*   Import `findspark` and initialize it. Then create a SparkContext.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XsF8mIFwSTG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName('wordCount')\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n"
      ],
      "metadata": {
        "id": "4S6cFFHjSQgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Create an RDD:**\n",
        "\n",
        "* Create a base RDD from a simple list of words and parallelize it."
      ],
      "metadata": {
        "id": "EFRbqnyRSVdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordsList = ['blue', 'orange', 'red', 'blue', 'red', 'blue']\n",
        "wordsRDD = sc.parallelize(wordsList, 5)\n",
        "print(type(wordsRDD))  # Should print <class 'pyspark.rdd.RDD'>\n"
      ],
      "metadata": {
        "id": "fH6XFPJtSZzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Transform the RDD:**\n",
        "\n",
        "* Use a map operation to transform each word in the RDD."
      ],
      "metadata": {
        "id": "5RGCwg2uSb-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a defined function\n",
        "def webify(x):\n",
        "    return x+\".com\"\n",
        "webified_RDD = wordsRDD.map(webify)\n",
        "print(webified_RDD.collect())\n",
        "\n",
        "# Using a lambda function\n",
        "webified_RDD = wordsRDD.map(lambda word: word + '.com')\n",
        "print(webified_RDD.collect())\n"
      ],
      "metadata": {
        "id": "ky5wMR8DSgc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Create Pair RDDs:**\n",
        "\n",
        "* Map each word to a tuple containing the word and the number 1, to prepare for counting."
      ],
      "metadata": {
        "id": "Z2R2uXnxSuZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordPairs = wordsRDD.map(lambda word: (word, 1))\n",
        "print(wordPairs.collect())\n"
      ],
      "metadata": {
        "id": "fG-l96ImSlsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Count the Words:**\n",
        "\n",
        "* Two approaches can be used: `groupByKey()` or `reduceByKey()`. The latter is more efficient."
      ],
      "metadata": {
        "id": "RIn4FGJrSn8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using groupByKey()\n",
        "wordsGrouped = wordPairs.groupByKey()\n",
        "wordCountsGrouped = wordsGrouped.mapValues(sum)\n",
        "print(wordCountsGrouped.collect())\n",
        "\n",
        "# Using reduceByKey()\n",
        "wordCounts = wordPairs.reduceByKey(lambda x, y: x + y)\n",
        "print(wordCounts.collect())\n"
      ],
      "metadata": {
        "id": "7f-4QNOySqzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Run the Complete Application:**\n",
        "\n",
        "* Combine all the steps into a single sequence to count the words."
      ],
      "metadata": {
        "id": "E3S7xvfXS0JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordCountsCollected = (wordsRDD\n",
        "                       .map(lambda word: (word, 1))\n",
        "                       .reduceByKey(lambda x, y: x + y)\n",
        "                       .collect())\n",
        "print(wordCountsCollected)\n"
      ],
      "metadata": {
        "id": "AQ4HAL7NS6YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Shutting Down:**\n",
        "\n",
        "* After you are done, stop the SparkContext to free up resources."
      ],
      "metadata": {
        "id": "oAiOlf-HS9ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "nXYlzfNVUw9s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}