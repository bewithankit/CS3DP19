{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCrSaT8UNRxH72jw4J0cjM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bewithankit/CS3DP19/blob/main/PySpark_vs_Dask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX3p_0Spj27G"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "r2iGMQYIj9kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "import time\n",
        "\n",
        "# Initialize Spark\n",
        "conf = SparkConf().setAppName('wordCount')\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Read the text file and perform word count\n",
        "linesRDD = sc.textFile(\"/content/ulysses.txt\")\n",
        "wordsRDD = linesRDD.flatMap(lambda line: line.split())\n",
        "wordCounts = wordsRDD.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect the results\n",
        "word_counts_collected = wordCounts.collect()\n",
        "\n",
        "# Stop timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the word counts\n",
        "print(\"\\nWord count:\\n\", word_counts_collected)\n",
        "\n",
        "# Print the time taken\n",
        "print(\"Time taken with PySpark: {} seconds\".format(end_time - start_time))\n",
        "\n",
        "# Stop Spark\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "uWws5wdBj_hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dask.distributed import Client\n",
        "import dask.bag as db\n",
        "import time\n",
        "\n",
        "# Initialize Dask Client\n",
        "client = Client()\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Read the text file into a Dask Bag and perform word count\n",
        "lines = db.read_text(\"/content/ulysses.txt\")\n",
        "word_counts = lines.map(str.split).flatten().frequencies(sort=True)\n",
        "\n",
        "# Compute the results\n",
        "word_counts_computed = word_counts.compute()\n",
        "\n",
        "# Stop timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the word counts\n",
        "print(word_counts_computed)\n",
        "\n",
        "# Print the time taken\n",
        "print(\"Time taken with Dask: {} seconds\".format(end_time - start_time))\n",
        "\n",
        "# Shut down the Dask client\n",
        "client.shutdown()\n"
      ],
      "metadata": {
        "id": "oaRFivWPkEAQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}